"""
–î–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ñ–∞–∫—Ç—á–µ–∫–∏–Ω–≥–∞ —Å –æ—Ç–ª–∞–¥–∫–æ–π
"""

import logging
import asyncio
import json
import time
import re
from typing import Any, Dict, Tuple, List, Optional
from dataclasses import dataclass
from urllib.parse import urlparse
from openai import AsyncOpenAI
from config import Config
from sources_config import sources_config

logger = logging.getLogger(__name__)

@dataclass
class DebugInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏"""
    stage1_time: float = 0
    stage2_time: float = 0
    sources_found: List[str] = None
    sources_count: int = 0
    reasoning: str = ""
    web_search_used: bool = False
    fallback_used: bool = False
    stage2_attempts: int = 0
    stage2_attempts: int = 0
    
    def __post_init__(self):
        if self.sources_found is None:
            self.sources_found = []

class TwoStageFilter:
    """–î–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π —Ñ–∞–∫—Ç—á–µ–∫–µ—Ä"""
    
    def __init__(self):
        self.client = AsyncOpenAI(api_key=Config.OPENAI_API_KEY)
        self.gpt5_available = True
        self.sources = sources_config
        self.fact_check_model = Config.FACT_CHECK_MODEL or "gpt-4o"
        self.web_search_effort = Config.WEB_SEARCH_EFFORT or "medium"
        
    async def analyze_message(self, text: str, channel_name: str) -> Tuple[str, str, Optional[DebugInfo]]:
        """
        –î–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (–∫–∞—Ç–µ–≥–æ—Ä–∏—è, –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π, –æ—Ç–ª–∞–¥–æ—á–Ω–∞—è_–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è)
        """
        if not text or len(text.strip()) < 10:
            return "—Å–∫—Ä—ã—Ç–æ", "–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ", None
            
        debug = DebugInfo() if Config.DEBUG_MODE else None
        
        try:
            # –≠–¢–ê–ü 1: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            start_time = time.time()
            sources, analysis = await self._stage1_select_sources(text, debug)
            if debug:
                debug.stage1_time = time.time() - start_time
                debug.sources_found = [src.get("domain") or src.get("url", "") for src in sources]
                debug.sources_count = len(sources)
                debug.reasoning = analysis.get("reasoning", "")

            # –ï—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –≥–ª—É–±–æ–∫–æ–≥–æ —Ñ–∞–∫—Ç—á–µ–∫–∏–Ω–≥–∞, –∑–∞–≤–µ—Ä—à–∞–µ–º –Ω–∞ —ç—Ç–∞–ø–µ 1
            if not analysis.get("requires_fact_check", True):
                category, comment = self._finalize_without_stage2(analysis)
                return category, comment, debug

            # –≠–¢–ê–ü 2: –§–∞–∫—Ç—á–µ–∫–∏–Ω–≥ –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
            start_time = time.time()
            category, comment = await self._stage2_fact_check(text, sources, analysis, debug)
            if debug:
                debug.stage2_time = time.time() - start_time
            
            return category, comment, debug
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            if debug:
                debug.fallback_used = True
                debug.reasoning = f"–û—à–∏–±–∫–∞: {str(e)}"
            return "–¥—Ä—É–≥–æ–µ", "", debug
    
    async def _stage1_select_sources(self, text: str, debug: Optional[DebugInfo]) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """
        –≠–¢–ê–ü 1: –£–º–Ω—ã–π –≤—ã–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        """
        logger.info("üîç –≠–¢–ê–ü 1: –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –≤—ã–±–æ—Ä–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")
        
        prompt = f"""
–¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –∫ —Ñ–∞–∫—Ç—á–µ–∫–∏–Ω–≥—É. –ò–∑—É—á–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏ —Ä–µ—à–∏, –Ω—É–∂–µ–Ω –ª–∏ –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ñ–∞–∫—Ç–æ–≤.

–°–æ–æ–±—â–µ–Ω–∏–µ: "{text}"

–ï—Å–ª–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω—É–∂–Ω–∞, –ø—Ä–µ–¥–ª–æ–∂–∏ –¥–æ {Config.MAX_SOURCE_DOMAINS} –Ω–∞–¥—ë–∂–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –ø—Ä–æ—Ñ–∏–ª—å–Ω—ã–µ –°–ú–ò, –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö), –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –º–æ–∂–Ω–æ –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è. –ï—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ—Ö–æ–∂–µ –Ω–∞ —à—É—Ç–∫—É, –ª–∏—á–Ω—É—é –∑–∞–º–µ—Ç–∫—É –∏–ª–∏ —Å–ø–∞–º ‚Äî —É–∫–∞–∂–∏, –ø–æ—á–µ–º—É –≤—Ç–æ—Ä–æ–π —ç—Ç–∞–ø –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.

–û—Ç–≤–µ—Ç—å —Å—Ç—Ä–æ–≥–æ JSON-–æ–±—ä–µ–∫—Ç–æ–º:
{{
  "needs_fact_check": true/false,
  "classification": "news/entertainment/personal/spam/other",
  "reasoning": "–∫—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ",
  "skip_reason": "–ø–æ—á–µ–º—É –º–æ–∂–Ω–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Ñ–∞–∫—Ç—á–µ–∫–∏–Ω–≥ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)",
  "source_candidates": [
    {{
      "name": "–Ω–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞",
      "url": "https://...",
      "domain": "example.com",
      "why": "–∑–∞—á–µ–º —ç—Ç–æ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫",
      "priority": 1
    }}
  ],
  "recommended_queries": ["–ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å 1", "–ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å 2"]
}}

–ü—Ä–∞–≤–∏–ª–∞:
- –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π –¥–æ–º–µ–Ω—ã; –µ—Å–ª–∏ —Ç–æ—á–Ω–æ–≥–æ URL –Ω–µ—Ç, –¥–∞–π –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏.
- –£—á–∏—Ç—ã–≤–∞–π –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏ –ª–æ–∫–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏.
- –î—É–±–ª–∏—Ä—É—é—â–∏–µ —Å–∞–π—Ç—ã –Ω–µ –≤–∫–ª—é—á–∞–π.
- –ï—Å–ª–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ –Ω—É–∂–Ω–∞, –≤—ã—Å—Ç–∞–≤—å "needs_fact_check": false –∏ –æ–±—ä—è—Å–Ω–∏ –≤ "skip_reason".
"""

        analysis: Optional[Dict[str, Any]] = None

        try:
            primary_response = await self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                max_completion_tokens=600,
                temperature=0.1,
                response_format={"type": "json_object"}
            )

            result_text = primary_response.choices[0].message.content.strip()
            logger.info(f"üìã –û—Ç–≤–µ—Ç —ç—Ç–∞–ø–∞ 1: {result_text}")

            analysis = self._parse_stage1_json(result_text)
            if analysis is None:
                analysis = self._build_analysis_from_text(result_text, text)
            if analysis is None:
                logger.info("‚ôªÔ∏è –≠–¢–ê–ü 1: –ø–æ–≤—Ç–æ—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å —Å —É–∫–æ—Ä–æ—á–µ–Ω–Ω–æ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π")
                analysis = await self._stage1_retry_prompt(text)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —ç—Ç–∞–ø–∞ 1: {e}")
            analysis = None

        if analysis is None:
            fallback_analysis = {
                "needs_fact_check": True,
                "classification": "other",
                "reasoning": "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏",
                "requires_fact_check": True
            }
            backup = self._build_backup_sources(text)
            fallback_analysis["normalized_sources"] = backup
            return backup, fallback_analysis

        raw_candidates = analysis.get("source_candidates", [])
        if not isinstance(raw_candidates, list):
            logger.info("üéØ –≠–¢–ê–ü 1: –º–æ–¥–µ–ª—å –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ –≤–∏–¥–µ %s", type(raw_candidates).__name__)
            logger.debug("üì¶ –°—ã—Ä—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ: %r", raw_candidates)
            raw_candidates = []
        else:
            logger.info("üéØ –≠–¢–ê–ü 1: –º–æ–¥–µ–ª—å –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∞ %s —Å—ã—Ä—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤", len(raw_candidates))
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug("üì¶ –°—ã—Ä—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏: %r", raw_candidates[:5])

        normalized_sources = self._normalize_candidates(raw_candidates)
        if normalized_sources:
            logger.info("‚úÖ –≠–¢–ê–ü 1: –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –æ—Å—Ç–∞–ª–æ—Å—å %s –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤", len(normalized_sources))
        else:
            logger.info("‚ö†Ô∏è –≠–¢–ê–ü 1: –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å")
        analysis["normalized_sources"] = normalized_sources

        requires_fact_check = self._needs_fact_check(analysis)
        analysis["requires_fact_check"] = requires_fact_check

        if not requires_fact_check:
            logger.info("‚úÖ –≠–¢–ê–ü 1 –∑–∞–≤–µ—Ä—à–µ–Ω: —Ñ–∞–∫—Ç—á–µ–∫–∏–Ω–≥ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")
            return [], analysis

        if not normalized_sources:
            logger.info("‚ÑπÔ∏è –≠–¢–ê–ü 1: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–æ–±—Ä–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –Ω–∞–±–æ—Ä")
            backup = self._build_backup_sources(text)
            analysis["normalized_sources"] = backup
            normalized_sources = backup

        logger.info(f"‚úÖ –≠–¢–ê–ü 1 –∑–∞–≤–µ—Ä—à–µ–Ω: –≤—ã–±—Ä–∞–Ω–æ {len(normalized_sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
        return normalized_sources, analysis

    async def _stage2_fact_check(
        self,
        text: str,
        sources: List[Dict[str, Any]],
        analysis: Dict[str, Any],
        debug: Optional[DebugInfo]
    ) -> Tuple[str, str]:
        """
        –≠–¢–ê–ü 2: –§–∞–∫—Ç—á–µ–∫–∏–Ω–≥ –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        """
        logger.info(f"üìä –≠–¢–ê–ü 2: –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–∫—Ç—ã –ø–æ {len(sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º...")

        if not sources:
            # –ï—Å–ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–µ –Ω—É–∂–Ω—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–ø–∞–º), –¥–µ–ª–∞–µ–º –±—ã—Å—Ç—Ä—É—é –ø—Ä–æ–≤–µ—Ä–∫—É
            return await self._quick_spam_check(text, debug)

        attempts = self._build_stage2_attempts(sources)
        last_error: Optional[Exception] = None

        for idx, attempt_sources in enumerate(attempts, start=1):
            logger.info(
                "üß™ –≠–¢–ê–ü 2: –ø–æ–ø—ã—Ç–∫–∞ %s —Å %s –¥–æ–º–µ–Ω–∞–º–∏", idx, len(attempt_sources)
            )

            if debug:
                debug.stage2_attempts += 1

            try:
                return await self._run_stage2_attempt(
                    text,
                    attempt_sources,
                    Config.FACT_CHECK_TIMEOUT,
                    analysis,
                    debug
                )
            except asyncio.TimeoutError:
                last_error = asyncio.TimeoutError()
                preview = [src.get("domain") or self._extract_domain(src.get("url")) or "?" for src in attempt_sources[:3]]
                preview_text = ", ".join(filter(None, preview))
                if len(attempt_sources) > 3:
                    preview_text += "..."
                logger.warning(
                    "‚è∞ –¢–∞–π–º–∞—É—Ç –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ %s —ç—Ç–∞–ø–∞ 2 (–¥–æ–º–µ–Ω—ã: %s)",
                    idx,
                    preview_text or "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
                )
                if debug:
                    base_reason = debug.reasoning if debug.reasoning else "–õ–æ–≥–∏–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"
                    debug.reasoning = f"{base_reason} (timeout –ø–æ–ø—ã—Ç–∫–∞ {idx})"
                continue
            except Exception as e:
                last_error = e
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —ç—Ç–∞–ø–∞ 2 –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {idx}: {e}")
                if "gpt-5" in str(e).lower() and self.gpt5_available:
                    logger.info("GPT-5 –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ GPT-4o")
                    self.gpt5_available = False
                    return await self._stage2_fact_check(text, sources, debug)
                if debug:
                    base_reason = debug.reasoning if debug.reasoning else "–õ–æ–≥–∏–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"
                    debug.reasoning = f"{base_reason} (–æ—à–∏–±–∫–∞ —ç—Ç–∞–ø–∞ 2, –ø–æ–ø—ã—Ç–∫–∞ {idx})"
                continue

        logger.warning("‚ö†Ô∏è –≠—Ç–∞–ø 2 –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –≤ fallback")
        if debug:
            debug.fallback_used = True
        if isinstance(last_error, asyncio.TimeoutError):
            if debug:
                base_reason = debug.reasoning if debug.reasoning else "–õ–æ–≥–∏–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"
                debug.reasoning = f"{base_reason} (stage2 timeout)"
        return await self._fallback_check(text, debug)
    
    async def _quick_spam_check(self, text: str, debug: Optional[DebugInfo]) -> Tuple[str, str]:
        """–ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ø–∞–º –±–µ–∑ –≤–µ–±-–ø–æ–∏—Å–∫–∞"""
        logger.info("‚ö° –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ø–∞–º...")
        
        try:
            response = await self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{
                    "role": "user", 
                    "content": f"–≠—Ç–æ —Å–ø–∞–º/—Ä–µ–∫–ª–∞–º–∞/–º—É—Å–æ—Ä? –û—Ç–≤–µ—Ç—å –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º (–¥–∞/–Ω–µ—Ç): {text[:200]}"
                }],
                max_completion_tokens=10,
                temperature=0.1
            )
            
            answer = response.choices[0].message.content.strip().lower()
            
            if "–¥–∞" in answer or "—Å–ø–∞–º" in answer:
                return "—Å–∫—Ä—ã—Ç–æ", "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –∫–∞–∫ —Å–ø–∞–º/—Ä–µ–∫–ª–∞–º–∞"
            else:
                return "–¥—Ä—É–≥–æ–µ", ""
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏: {e}")
            return "–¥—Ä—É–≥–æ–µ", ""
    
    async def _fallback_check(self, text: str, debug: Optional[DebugInfo]) -> Tuple[str, str]:
        """–†–µ–∑–µ—Ä–≤–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞"""
        logger.info("üîÑ –†–µ–∑–µ—Ä–≤–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞...")
        
        try:
            response = await self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{
                    "role": "user", 
                    "content": f"""
–ö—Ä–∞—Ç–∫–æ –æ—Ü–µ–Ω–∏ —ç—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ:
"{text}"

–≠—Ç–æ: 1) —Å–ø–∞–º/–º—É—Å–æ—Ä 2) –Ω–æ–≤–æ—Å—Ç–∏ 3) —Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏—è 4) –¥—Ä—É–≥–æ–µ
–û—Ç–≤–µ—Ç—å –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π: –∫–∞—Ç–µ–≥–æ—Ä–∏—è | –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π (–µ—Å–ª–∏ –Ω—É–∂–µ–Ω)
"""
                }],
                max_completion_tokens=50,
                temperature=0.1
            )
            
            answer = response.choices[0].message.content.strip()
            answer_lower = answer.lower()
            manual_review = "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏, —Ç—Ä–µ–±—É–µ—Ç—Å—è —Ä—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞"

            if "—Å–ø–∞–º" in answer_lower or "–º—É—Å–æ—Ä" in answer_lower:
                return "—Å–∫—Ä—ã—Ç–æ", "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –∫–∞–∫ —Å–ø–∞–º"

            if "—Ä–∞–∑–≤–ª–µ—á" in answer_lower:
                return "—Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏—è", manual_review

            if "–Ω–æ–≤–æ—Å—Ç" in answer_lower:
                return "–Ω–æ–≤–æ—Å—Ç–∏", manual_review

            if "–¥—Ä—É–≥–æ" in answer_lower:
                return "–¥—Ä—É–≥–æ–µ", manual_review

            return "–¥—Ä—É–≥–æ–µ", manual_review
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏: {e}")
            return "–¥—Ä—É–≥–æ–µ", "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏, —Ç—Ä–µ–±—É–µ—Ç—Å—è —Ä—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞"

    async def _run_stage2_attempt(
        self,
        text: str,
        attempt_sources: List[Dict[str, Any]],
        timeout: float,
        analysis: Optional[Dict[str, Any]],
        debug: Optional[DebugInfo]
    ) -> Tuple[str, str]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–¥–∏–Ω–æ—á–Ω—É—é –ø–æ–ø—ã—Ç–∫—É —ç—Ç–∞–ø–∞ 2 —Å –∑–∞–¥–∞–Ω–Ω—ã–º —Å–ø–∏—Å–∫–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤."""

        sources_text = self._format_sources_for_prompt(attempt_sources)

        queries = analysis.get("recommended_queries") if analysis else None
        queries_text = ""
        if queries:
            prepared = [q for q in queries[:3] if isinstance(q, str) and q.strip()]
            if prepared:
                bullet_list = "\n".join([f"‚Ä¢ {q.strip()}" for q in prepared])
                queries_text = f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã:\n{bullet_list}\n\n"

        allowed_domains = [
            src.get("domain") or self._extract_domain(src.get("url"))
            for src in attempt_sources
        ]
        allowed_domains = [d for d in allowed_domains if d]

        prompt = f"""
–ü—Ä–æ–≤–µ—Ä—å –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å —ç—Ç–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è –≤–µ–±-–ø–æ–∏—Å–∫ –¢–û–õ–¨–ö–û –ø–æ —É–∫–∞–∑–∞–Ω–Ω—ã–º –Ω–∞–¥—ë–∂–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º.

–ò—Å—Ç–æ—á–Ω–∏–∫–∏:
{sources_text}

{queries_text}
–°–æ–æ–±—â–µ–Ω–∏–µ: "{text}"

–î–µ–π—Å—Ç–≤–∏—è:
1. –ù–∞–π–¥–∏ —Ñ–∞–∫—Ç—ã —á–µ—Ä–µ–∑ –≤–µ–±-–ø–æ–∏—Å–∫ –ø–æ —Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–º –¥–æ–º–µ–Ω–∞–º.
2. –û–ø—Ä–µ–¥–µ–ª–∏, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å–ø–∞–º–æ–º –∏–ª–∏ —Å–æ–º–Ω–∏—Ç–µ–ª—å–Ω—ã–º.
3. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–ø–æ—Ä–Ω–∞—è ‚Äî –¥–∞–π –∫—Ä–∞—Ç–∫–∏–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π.
4. –û–ø—Ä–µ–¥–µ–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—é: –Ω–æ–≤–æ—Å—Ç–∏ / —Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏—è / –¥—Ä—É–≥–æ–µ.
5. –í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å—Ç—Ä–æ–≥–æ –≤ JSON.

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
{{
  "is_garbage": true/false,
  "is_questionable": true/false,
  "category": "–Ω–æ–≤–æ—Å—Ç–∏/—Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏—è/–¥—Ä—É–≥–æ–µ",
  "reason": "–ø—Ä–∏—á–∏–Ω–∞ –µ—Å–ª–∏ –º—É—Å–æ—Ä",
  "comment": "–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –µ—Å–ª–∏ —Å–æ–º–Ω–∏—Ç–µ–ª—å–Ω–æ (–¥–æ 120 —Å–∏–º–≤–æ–ª–æ–≤)",
  "sources_checked": ["—Å–ø–∏—Å–æ–∫ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"]
}}
"""

        responses_client = self.client.responses

        try:
            create_task = responses_client.create(
                model=self.fact_check_model,
                tools=[{
                    "type": "web_search",
                    "filters": {
                        "allowed_domains": allowed_domains
                    }
                }],
                input=prompt,
                tool_choice="auto",
                max_output_tokens=600
            )
            initial_response = await asyncio.wait_for(create_task, timeout=timeout)
            response = await self._poll_response(responses_client, initial_response, timeout)
        except asyncio.TimeoutError:
            raise
        except Exception as err:
            if "model" in str(err).lower() and "not supported" in str(err).lower():
                logger.warning(
                    "‚ö†Ô∏è –ú–æ–¥–µ–ª—å %s –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Responses API, –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ gpt-4o",
                    self.fact_check_model
                )
                self.fact_check_model = "gpt-4o"
                create_task = responses_client.create(
                    model=self.fact_check_model,
                    tools=[{
                        "type": "web_search",
                        "filters": {
                            "allowed_domains": allowed_domains
                        }
                    }],
                    input=prompt,
                    tool_choice="auto",
                    max_output_tokens=600
                )
                initial_response = await asyncio.wait_for(create_task, timeout=timeout)
                response = await self._poll_response(responses_client, initial_response, timeout)
            else:
                raise

        if debug:
            debug.web_search_used = True

        output_text = self._extract_response_text(response)
        status = getattr(response, "status", None)
        logger.debug("üì∂ –°—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞ —ç—Ç–∞–ø–∞ 2: %s", status)
        if logger.isEnabledFor(logging.DEBUG):
            try:
                logger.debug("üìù –ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç —ç—Ç–∞–ø–∞ 2: %s", response.model_dump(exclude_none=True))
            except Exception:
                logger.debug("üìù –ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç —ç—Ç–∞–ø–∞ 2: %r", response)
        logger.info(f"üìÑ –û—Ç–≤–µ—Ç —ç—Ç–∞–ø–∞ 2: {output_text[:200]}...")

        if not output_text:
            raise ValueError("–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏ —ç—Ç–∞–ø–∞ 2")

        try:
            result = json.loads(output_text)
        except json.JSONDecodeError:
            json_start = output_text.find('{')
            json_end = output_text.rfind('}') + 1
            if json_start == -1 or json_end <= json_start:
                raise json.JSONDecodeError("JSON –Ω–µ –Ω–∞–π–¥–µ–Ω", output_text, 0)
            json_text = output_text[json_start:json_end]
            result = json.loads(json_text)

        if result.get("is_garbage", False):
            return "—Å–∫—Ä—ã—Ç–æ", result.get("reason", "") or "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –∫–∞–∫ —Å–ø–∞–º"

        if result.get("is_questionable", False):
            category = result.get("category", "–¥—Ä—É–≥–æ–µ")
            comment = result.get("comment", "") or "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏, —Ç—Ä–µ–±—É–µ—Ç—Å—è —Ä—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞"
            return category, comment

        category = result.get("category", "–¥—Ä—É–≥–æ–µ")
        comment = result.get("comment", "")
        return category, comment

    def _build_stage2_attempts(self, sources: List[Dict[str, Any]]) -> List[List[Dict[str, Any]]]:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ–ø—ã—Ç–æ–∫ –¥–ª—è —ç—Ç–∞–ø–∞ 2 —Å —Ä–∞–∑–Ω—ã–º–∏ –ª–∏–º–∏—Ç–∞–º–∏ –¥–æ–º–µ–Ω–æ–≤."""

        unique_sources: List[Dict[str, Any]] = []
        seen = set()
        for candidate in sources:
            domain = candidate.get("domain") or self._extract_domain(candidate.get("url"))
            url = candidate.get("url") or ""
            if not domain and not url:
                continue
            key = (domain or "", url)
            if key in seen:
                continue
            seen.add(key)
            normalized = dict(candidate)
            normalized["domain"] = domain
            if url:
                normalized["url"] = url
            elif domain:
                normalized["url"] = f"https://{domain}"
            unique_sources.append(normalized)

        if not unique_sources:
            return [[]]

        limits: List[int] = []
        if Config.STAGE2_INITIAL_DOMAIN_LIMIT:
            limits.append(Config.STAGE2_INITIAL_DOMAIN_LIMIT)
        if Config.STAGE2_RETRY_DOMAIN_LIMIT and Config.STAGE2_RETRY_DOMAIN_LIMIT != Config.STAGE2_INITIAL_DOMAIN_LIMIT:
            limits.append(Config.STAGE2_RETRY_DOMAIN_LIMIT)
        limits.append(len(unique_sources))

        attempts: List[List[Dict[str, Any]]] = []
        for limit in limits:
            if limit is None or limit <= 0:
                continue
            trimmed = unique_sources[:limit]
            if not trimmed:
                continue
            if trimmed not in attempts:
                attempts.append(trimmed)

        if not attempts:
            attempts.append(unique_sources)
        elif attempts[-1] != unique_sources:
            attempts.append(unique_sources)

        return attempts

    def _needs_fact_check(self, analysis: Dict[str, Any]) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤—Ç–æ—Ä–æ–≥–æ —ç—Ç–∞–ø–∞ –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –∞–Ω–∞–ª–∏–∑–∞ —ç—Ç–∞–ø–∞ 1."""
        if "needs_fact_check" in analysis:
            return bool(analysis.get("needs_fact_check"))

        topic_type = (analysis.get("classification") or analysis.get("topic_type") or "").lower()

        if topic_type in {"spam", "entertainment"}:
            return False

        if topic_type == "personal":
            return False

        return True

    def _finalize_without_stage2(self, analysis: Dict[str, Any]) -> Tuple[str, str]:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç –∏—Ç–æ–≥ –±–µ–∑ –∑–∞–ø—É—Å–∫–∞ –≤—Ç–æ—Ä–æ–≥–æ —ç—Ç–∞–ø–∞ —Ñ–∞–∫—Ç—á–µ–∫–∏–Ω–≥–∞."""
        classification = (analysis.get("classification") or analysis.get("topic_type") or "").lower()
        skip_reason = analysis.get("skip_reason") or analysis.get("reasoning") or "–ù–µ —Ç—Ä–µ–±—É–µ—Ç —Ñ–∞–∫—Ç—á–µ–∫–∏–Ω–≥–∞"

        if classification == "spam":
            return "—Å–∫—Ä—ã—Ç–æ", skip_reason or "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –∫–∞–∫ —Å–ø–∞–º"

        if classification == "entertainment":
            return "—Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏—è", skip_reason if skip_reason else ""

        if classification == "personal":
            return "–¥—Ä—É–≥–æ–µ", skip_reason or "–ù–µ–ø—Ä–æ–≤–µ—Ä—è–µ–º–æ–µ –ª–∏—á–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ"

        if classification == "news":
            return "–Ω–æ–≤–æ—Å—Ç–∏", skip_reason or "–ù–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏"

        return "–¥—Ä—É–≥–æ–µ", skip_reason or "–ù–µ —Ç—Ä–µ–±—É–µ—Ç —Ñ–∞–∫—Ç—á–µ–∫–∏–Ω–≥–∞"

    def _normalize_candidates(self, candidates: Any) -> List[Dict[str, Any]]:
        """–ü—Ä–∏–≤–æ–¥–∏—Ç —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∫ –µ–¥–∏–Ω–æ–º—É –≤–∏–¥—É."""

        if not isinstance(candidates, list):
            return []

        normalized: List[Dict[str, Any]] = []

        for item in candidates:
            if isinstance(item, dict):
                url = item.get("url") or item.get("link") or ""
                domain = item.get("domain") or self._extract_domain(url)
                if not domain and not url:
                    continue
                name = item.get("name") or domain or url
                why = item.get("why") or item.get("reason") or item.get("note", "")
                priority = item.get("priority")
                try:
                    priority = int(priority)
                except (TypeError, ValueError):
                    priority = len(normalized) + 1
                normalized.append({
                    "name": name,
                    "url": url or (f"https://{domain}" if domain else ""),
                    "domain": domain,
                    "why": why,
                    "priority": priority
                })
            elif isinstance(item, str):
                domain = self._extract_domain(item)
                url = item if item.startswith("http") else (f"https://{item}" if domain else "")
                normalized.append({
                    "name": domain or item,
                    "url": url,
                    "domain": domain,
                    "why": "–ò—Å—Ç–æ—á–Ω–∏–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–æ–¥–µ–ª—å—é",
                    "priority": len(normalized) + 1
                })

        filtered = [src for src in normalized if src.get("domain") or src.get("url")]
        filtered.sort(key=lambda s: s.get("priority", 999))
        return filtered[:Config.MAX_SOURCE_DOMAINS]

    def _build_backup_sources(self, text: str) -> List[Dict[str, Any]]:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç —Ä–µ–∑–µ—Ä–≤–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""

        fallback_domains = list(self.sources.get_category_domains("general_news"))
        fallback_domains += list(self.sources.get_sources_for_topic(text))

        unique: List[Dict[str, Any]] = []
        seen = set()
        for domain in fallback_domains:
            if not domain or domain in seen:
                continue
            seen.add(domain)
            unique.append({
                "name": domain,
                "url": f"https://{domain}",
                "domain": domain,
                "why": "–†–µ–∑–µ—Ä–≤–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫",
                "priority": len(unique) + 1
            })

        return unique[:Config.MAX_SOURCE_DOMAINS]

    def _extract_domain(self, value: Optional[str]) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ–º–µ–Ω –∏–∑ URL –∏–ª–∏ —Å—ã—Ä–æ–π —Å—Ç—Ä–æ–∫–∏."""

        if not value:
            return None

        candidate = value.strip()
        if not candidate or " " in candidate:
            return None

        parsed = urlparse(candidate if candidate.startswith("http") else f"https://{candidate}")
        domain = parsed.netloc.lower()
        if domain.startswith("www."):
            domain = domain[4:]
        return domain or None

    def _format_sources_for_prompt(self, sources: List[Dict[str, Any]]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ –º–æ–¥–µ–ª—å."""

        lines: List[str] = []
        for src in sources[:15]:
            name = src.get("name") or src.get("domain") or src.get("url")
            url = src.get("url") or (f"https://{src.get('domain')}" if src.get("domain") else "")
            why = src.get("why")
            segments = [segment for segment in [name, url, why] if segment]
            if segments:
                lines.append("‚Ä¢ " + " ‚Äî ".join(segments))

        if not lines:
            return "‚Ä¢ (–∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã)"

        return "\n".join(lines)

    def _extract_response_text(self, response: Any) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ –æ–±—ä–µ–∫—Ç–∞ –æ—Ç–≤–µ—Ç–∞ Responses API."""

        direct_text = getattr(response, "output_text", None)
        if direct_text:
            return direct_text.strip()

        data: Dict[str, Any] = {}
        if hasattr(response, "model_dump"):
            try:
                data = response.model_dump(exclude_none=True)
            except Exception:
                data = {}
        elif hasattr(response, "dict"):
            try:
                data = response.dict(exclude_none=True)
            except Exception:
                data = {}

        chunks: List[str] = []
        for item in data.get("output", []) or []:
            if isinstance(item, dict):
                item_type = item.get("type")
                if item_type == "message":
                    content = item.get("content") or []
                    chunks.extend(self._extract_text_from_content(content))
                elif item_type == "tool_call":
                    tool = item.get("tool_call") or {}
                    output = tool.get("output") or tool.get("result") or tool.get("response")
                    chunks.extend(self._extract_text_from_tool_output(output))
            else:
                content = getattr(item, "content", None)
                if content:
                    chunks.extend(self._extract_text_from_content(content))

        if chunks:
            return "\n".join(chunks).strip()

        # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ ‚Äî –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–ª–æ –æ—Ç–≤–µ—Ç–∞ —Ü–µ–ª–∏–∫–æ–º
        raw = data.get("response") or data.get("output_text")
        if isinstance(raw, str) and raw.strip():
            return raw.strip()

        return ""

    def _extract_text_from_content(self, content: Any) -> List[str]:
        """–í—ã—Ç—è–≥–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ message."""

        segments: List[str] = []
        for part in content:
            if isinstance(part, dict):
                text_value = part.get("text")
                if text_value:
                    segments.append(text_value)
            else:
                text_value = getattr(part, "text", None)
                if text_value:
                    segments.append(text_value)
        return segments

    def _parse_stage1_json(self, payload: str) -> Optional[Dict[str, Any]]:
        """–ü—ã—Ç–∞–µ—Ç—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –æ—Ç–≤–µ—Ç–∞ —ç—Ç–∞–ø–∞ 1, –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—è –æ–±—Ä–µ–∑–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç."""

        if not payload:
            return None

        def try_load(candidate: str) -> Optional[Dict[str, Any]]:
            try:
                return json.loads(candidate)
            except json.JSONDecodeError:
                return None

        # –ü–µ—Ä–≤–∞—è –ø–æ–ø—ã—Ç–∫–∞ ‚Äî –∫–∞–∫ –µ—Å—Ç—å
        parsed = try_load(payload)
        if parsed is not None:
            return parsed

        # –ü–æ–ø—ã—Ç–∫–∞ –¥–æ–±–∞–≤–∏—Ç—å –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–µ —Å–∫–æ–±–∫–∏
        balanced = payload
        brace_diff = balanced.count('{') - balanced.count('}')
        if brace_diff > 0:
            balanced += '}' * brace_diff
        bracket_diff = balanced.count('[') - balanced.count(']')
        if bracket_diff > 0:
            balanced += ']' * bracket_diff

        parsed = try_load(balanced)
        if parsed is not None:
            logger.warning("‚ö†Ô∏è –≠–¢–ê–ü 1: –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏–ª–∏ JSON –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –∑–∞–∫—Ä—ã–≤–∞—é—â–∏—Ö —Å–∫–æ–±–æ–∫")
            return parsed

        # –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ –æ–±—Ä–µ–∑–∞–µ–º –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–π —Å–∫–æ–±–∫–∏
        for end in range(len(payload), 0, -1):
            if payload[end - 1] not in '}])\"':
                continue
            candidate = payload[:end]
            candidate = candidate.rstrip(',')
            candidate_parsed = try_load(candidate)
            if candidate_parsed is not None:
                logger.warning("‚ö†Ô∏è –≠–¢–ê–ü 1: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ —É—Å–µ—á—ë–Ω–Ω—ã–π JSON")
                return candidate_parsed

        logger.warning(
            "‚ö†Ô∏è –≠–¢–ê–ü 1: –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON. –û–±—Ä–µ–∑–∞–Ω–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç: %s",
            payload[:200]
        )
        return None

    def _build_analysis_from_text(self, payload: str, original_text: str) -> Optional[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —á–∞—Å—Ç–∏—á–Ω–æ –æ–±—Ä–µ–∑–∞–Ω–Ω–æ–≥–æ JSON."""

        if not payload:
            return None

        def capture(field: str) -> str:
            match = re.search(rf'"{field}"\s*:\s*"([^"]*)"', payload)
            return match.group(1) if match else ""

        classification = capture("classification") or "other"
        reasoning = capture("reasoning") or ""
        skip_reason = capture("skip_reason")

        candidate_pattern = re.compile(
            r'\{[^\}]*?"name"\s*:\s*"(?P<name>[^"]+)"[^\}]*?"url"\s*:\s*"(?P<url>[^"]+)"'
            r'[^\}]*?"domain"\s*:\s*"(?P<domain>[^"]+)"[^\}]*?"why"\s*:\s*"(?P<why>[^"]+)"'
            r'[^\}]*?"priority"\s*:\s*(?P<priority>\d+)',
            re.S
        )

        candidates: List[Dict[str, Any]] = []
        for match in candidate_pattern.finditer(payload):
            try:
                candidates.append({
                    "name": match.group("name"),
                    "url": match.group("url"),
                    "domain": match.group("domain"),
                    "why": match.group("why"),
                    "priority": int(match.group("priority"))
                })
            except Exception:
                continue

        if not candidates:
            return None

        analysis = {
            "needs_fact_check": True,
            "classification": classification,
            "reasoning": reasoning or "",
            "skip_reason": skip_reason,
            "source_candidates": candidates[:Config.MAX_SOURCE_DOMAINS],
            "recommended_queries": []
        }

        logger.warning("‚ö†Ô∏è –≠–¢–ê–ü 1: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ —ç–≤—Ä–∏—Å—Ç–∏–∫—É –¥–ª—è —Ä–∞–∑–±–æ—Ä–∞ —É—Å–µ—á—ë–Ω–Ω–æ–≥–æ JSON")
        return analysis

    def _extract_text_from_tool_output(self, output: Any) -> List[str]:
        """–î–æ—Å—Ç–∞–µ—Ç —á–∏—Ç–∞–µ–º—ã–π —Ç–µ–∫—Å—Ç –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞."""

        segments: List[str] = []
        if isinstance(output, str):
            segments.append(output)
        elif isinstance(output, list):
            for item in output:
                if isinstance(item, dict):
                    if item.get("type") == "text" and item.get("text"):
                        segments.append(item["text"])
                    elif item.get("title") or item.get("url") or item.get("snippet"):
                        title = item.get("title")
                        snippet = item.get("snippet")
                        url = item.get("url")
                        pieces = [piece for piece in [title, snippet, url] if piece]
                        if pieces:
                            segments.append(" ‚Äî ".join(pieces))
                elif isinstance(item, str):
                    segments.append(item)
        elif isinstance(output, dict):
            if output.get("text"):
                segments.append(output["text"])
            if output.get("content"):
                segments.extend(self._extract_text_from_tool_output(output["content"]))
        return segments

    async def _poll_response(self, responses_client, response: Any, timeout: float) -> Any:
        """–û–∂–∏–¥–∞–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è Responses API —Å —Ç–∞–π–º–∞—É—Ç–æ–º."""

        start = time.time()
        current = response

        while getattr(current, "status", "completed") in {"in_progress", "queued", "requires_action"}:
            remaining = timeout - (time.time() - start)
            if remaining <= 0:
                raise asyncio.TimeoutError("Polling timed out")
            await asyncio.sleep(min(1.5, remaining))
            current = await responses_client.get(current.id)

        return current

    async def _stage1_retry_prompt(self, text: str) -> Optional[Dict[str, Any]]:
        """–ü–æ–≤—Ç–æ—Ä–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è —ç—Ç–∞–ø–∞ 1 —Å —É–ø—Ä–æ—â—ë–Ω–Ω—ã–º–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏."""

        retry_prompt = f"""
–í–µ—Ä–Ω–∏ –≤–∞–ª–∏–¥–Ω—ã–π JSON (–¥–æ 6 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤) –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤ –ø–æ —Å–æ–æ–±—â–µ–Ω–∏—é:
"{text}"

–°—Ç—Ä–æ–≥–æ —Å–ª–µ–¥—É–π —Ñ–æ—Ä–º–∞—Ç—É:
{{
  "needs_fact_check": true/false,
  "classification": "news/entertainment/personal/spam/other",
  "reasoning": "...",
  "skip_reason": "...",
  "source_candidates": [
    {{"name": "...", "url": "https://...", "domain": "...", "why": "...", "priority": 1}}
  ],
  "recommended_queries": ["..."]
}}

–ù–∏–∫–∞–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤–Ω–µ JSON.
"""

        try:
            response = await self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": retry_prompt}],
                max_completion_tokens=400,
                temperature=0.0,
                response_format={"type": "json_object"}
            )
            result_text = response.choices[0].message.content.strip()
            logger.info(f"üìã –û—Ç–≤–µ—Ç —ç—Ç–∞–ø–∞ 1 (retry): {result_text}")
            analysis = self._parse_stage1_json(result_text)
            if analysis is None:
                analysis = self._build_analysis_from_text(result_text, text)
            return analysis
        except Exception as err:
            logger.error(f"‚ùå –≠–¢–ê–ü 1 retry –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –æ—à–∏–±–∫–æ–π: {err}")
            return None
