"""
–î–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ñ–∞–∫—Ç—á–µ–∫–∏–Ω–≥–∞ —Å –æ—Ç–ª–∞–¥–∫–æ–π
"""

import logging
import asyncio
import json
import time
import re
from datetime import datetime
from typing import Any, Dict, Tuple, List, Optional
from dataclasses import dataclass
from urllib.parse import urlparse
from openai import AsyncOpenAI
from config import Config
from sources_config import sources_config

logger = logging.getLogger(__name__)

@dataclass
class DebugInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏"""
    stage1_time: float = 0
    stage2_time: float = 0
    sources_found: List[str] = None
    sources_count: int = 0
    reasoning: str = ""
    web_search_used: bool = False
    fallback_used: bool = False
    stage2_attempts: int = 0
    confidence_score: int = 0
    verification_status: str = ""
    detailed_findings: str = ""
    contradictions: str = ""
    missing_evidence: str = ""
    special_notes: str = ""
    
    def __post_init__(self):
        if self.sources_found is None:
            self.sources_found = []

class TwoStageFilter:
    """–î–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π —Ñ–∞–∫—Ç—á–µ–∫–µ—Ä"""
    
    def __init__(self):
        self.client = AsyncOpenAI(api_key=Config.OPENAI_API_KEY)
        self.gpt5_available = True
        self.sources = sources_config
        self.fact_check_model = Config.FACT_CHECK_MODEL or "gpt-4o"
        self.web_search_effort = Config.WEB_SEARCH_EFFORT or "medium"
        
    async def analyze_message(self, text: str, channel_name: str) -> Tuple[str, str, Optional[DebugInfo]]:
        """
        –î–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (–∫–∞—Ç–µ–≥–æ—Ä–∏—è, –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π, –æ—Ç–ª–∞–¥–æ—á–Ω–∞—è_–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è)
        """
        if not text or len(text.strip()) < 10:
            return "—Å–∫—Ä—ã—Ç–æ", "–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ", None
            
        debug = DebugInfo() if Config.DEBUG_MODE else None
        
        try:
            # –≠–¢–ê–ü 1: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            start_time = time.time()
            sources, analysis = await self._stage1_select_sources(text, debug)
            if debug:
                debug.stage1_time = time.time() - start_time
                debug.sources_found = [src.get("domain") or src.get("url", "") for src in sources]
                debug.sources_count = len(sources)
                debug.reasoning = analysis.get("reasoning", "")

            # –ï—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –≥–ª—É–±–æ–∫–æ–≥–æ —Ñ–∞–∫—Ç—á–µ–∫–∏–Ω–≥–∞, –∑–∞–≤–µ—Ä—à–∞–µ–º –Ω–∞ —ç—Ç–∞–ø–µ 1
            if not analysis.get("requires_fact_check", True):
                category, comment = self._finalize_without_stage2(analysis)
                return category, comment, debug

            # –≠–¢–ê–ü 2: –§–∞–∫—Ç—á–µ–∫–∏–Ω–≥ –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
            start_time = time.time()
            category, comment = await self._stage2_fact_check(text, sources, analysis, debug)
            if debug:
                debug.stage2_time = time.time() - start_time
            
            return category, comment, debug
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            if debug:
                debug.fallback_used = True
                debug.reasoning = f"–û—à–∏–±–∫–∞: {str(e)}"
            return "–¥—Ä—É–≥–æ–µ", "", debug
    
    async def _stage1_select_sources(self, text: str, debug: Optional[DebugInfo]) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """
        –≠–¢–ê–ü 1: –£–º–Ω—ã–π –≤—ã–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        """
        logger.info("üîç STAGE 1: Analyzing text for source selection...")
        
        current_year = datetime.now().year
        prompt = f"""
–¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –∫ —Ñ–∞–∫—Ç—á–µ–∫–∏–Ω–≥—É. –ò–∑—É—á–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏ —Ä–µ—à–∏, –Ω—É–∂–µ–Ω –ª–∏ –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ñ–∞–∫—Ç–æ–≤.

–í–ê–ñ–ù–û: –°–µ–π—á–∞—Å {current_year} –≥–æ–¥. –ò—Å–ø–æ–ª—å–∑—É–π –∞–∫—Ç—É–∞–ª—å–Ω—ã–π –≥–æ–¥ –≤ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö.

–°–æ–æ–±—â–µ–Ω–∏–µ: "{text}"

–ï—Å–ª–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω—É–∂–Ω–∞, –ø—Ä–µ–¥–ª–æ–∂–∏ –¥–æ {Config.MAX_SOURCE_DOMAINS} –Ω–∞–¥—ë–∂–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –ø—Ä–æ—Ñ–∏–ª—å–Ω—ã–µ –°–ú–ò, –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö), –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –º–æ–∂–Ω–æ –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è. –ï—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ—Ö–æ–∂–µ –Ω–∞ —à—É—Ç–∫—É, –ª–∏—á–Ω—É—é –∑–∞–º–µ—Ç–∫—É –∏–ª–∏ —Å–ø–∞–º ‚Äî —É–∫–∞–∂–∏, –ø–æ—á–µ–º—É –≤—Ç–æ—Ä–æ–π —ç—Ç–∞–ø –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.

–û—Ç–≤–µ—Ç—å —Å—Ç—Ä–æ–≥–æ JSON-–æ–±—ä–µ–∫—Ç–æ–º:
{{
  "needs_fact_check": true/false,
  "classification": "news/entertainment/personal/spam/other",
  "reasoning": "–∫—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ",
  "skip_reason": "–ø–æ—á–µ–º—É –º–æ–∂–Ω–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Ñ–∞–∫—Ç—á–µ–∫–∏–Ω–≥ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)",
  "source_candidates": [
    {{
      "name": "–Ω–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞",
      "url": "https://...",
      "domain": "example.com",
      "why": "–∑–∞—á–µ–º —ç—Ç–æ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫",
      "priority": 1
    }}
  ],
  "recommended_queries": ["–ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å 1", "–ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å 2"]
}}

–ü—Ä–∞–≤–∏–ª–∞:
- –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π –¥–æ–º–µ–Ω—ã; –µ—Å–ª–∏ —Ç–æ—á–Ω–æ–≥–æ URL –Ω–µ—Ç, –¥–∞–π –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏.
- –£—á–∏—Ç—ã–≤–∞–π –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏ –ª–æ–∫–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏.
- –î—É–±–ª–∏—Ä—É—é—â–∏–µ —Å–∞–π—Ç—ã –Ω–µ –≤–∫–ª—é—á–∞–π.
- –í –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö –∏—Å–ø–æ–ª—å–∑—É–π –∞–∫—Ç—É–∞–ª—å–Ω—ã–π {current_year} –≥–æ–¥ –≤–º–µ—Å—Ç–æ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –¥–∞—Ç.
- –ï—Å–ª–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ –Ω—É–∂–Ω–∞, –≤—ã—Å—Ç–∞–≤—å "needs_fact_check": false –∏ –æ–±—ä—è—Å–Ω–∏ –≤ "skip_reason".
"""

        analysis: Optional[Dict[str, Any]] = None

        try:
            primary_response = await self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                max_completion_tokens=Config.STAGE1_MAX_TOKENS,
                temperature=0.1,
                response_format={"type": "json_object"}
            )

            result_text = primary_response.choices[0].message.content.strip()
            logger.info(f"üìã Stage 1 response: {result_text}")

            analysis = self._parse_stage1_json(result_text)
            if analysis is None:
                logger.info("‚ôªÔ∏è STAGE 1: retrying with shortened prompt")
                analysis = await self._stage1_retry_prompt(text)
        except Exception as e:
            logger.error(f"‚ùå Stage 1 error: {e}")
            analysis = None

        if analysis is None:
            fallback_analysis = {
                "needs_fact_check": True,
                "classification": "other",
                "reasoning": "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏",
                "requires_fact_check": True
            }
            backup = self._build_backup_sources(text)
            fallback_analysis["normalized_sources"] = backup
            return backup, fallback_analysis

        raw_candidates = analysis.get("source_candidates", [])
        if not isinstance(raw_candidates, list):
            logger.info("üéØ –≠–¢–ê–ü 1: –º–æ–¥–µ–ª—å –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ –≤–∏–¥–µ %s", type(raw_candidates).__name__)
            logger.debug("üì¶ –°—ã—Ä—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ: %r", raw_candidates)
            raw_candidates = []
        else:
            logger.info("üéØ –≠–¢–ê–ü 1: –º–æ–¥–µ–ª—å –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∞ %s —Å—ã—Ä—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤", len(raw_candidates))
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug("üì¶ –°—ã—Ä—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏: %r", raw_candidates[:5])

        normalized_sources = self._normalize_candidates(raw_candidates)
        if normalized_sources:
            logger.info("‚úÖ –≠–¢–ê–ü 1: –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –æ—Å—Ç–∞–ª–æ—Å—å %s –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤", len(normalized_sources))
        else:
            logger.info("‚ö†Ô∏è –≠–¢–ê–ü 1: –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å")
        analysis["normalized_sources"] = normalized_sources

        requires_fact_check = self._needs_fact_check(analysis)
        analysis["requires_fact_check"] = requires_fact_check

        if not requires_fact_check:
            logger.info("‚úÖ –≠–¢–ê–ü 1 –∑–∞–≤–µ—Ä—à–µ–Ω: —Ñ–∞–∫—Ç—á–µ–∫–∏–Ω–≥ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")
            return [], analysis

        if not normalized_sources:
            logger.info("‚ÑπÔ∏è –≠–¢–ê–ü 1: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–æ–±—Ä–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –Ω–∞–±–æ—Ä")
            backup = self._build_backup_sources(text)
            analysis["normalized_sources"] = backup
            normalized_sources = backup

        logger.info(f"‚úÖ –≠–¢–ê–ü 1 –∑–∞–≤–µ—Ä—à–µ–Ω: –≤—ã–±—Ä–∞–Ω–æ {len(normalized_sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
        return normalized_sources, analysis

    async def _stage2_fact_check(
        self,
        text: str,
        sources: List[Dict[str, Any]],
        analysis: Dict[str, Any],
        debug: Optional[DebugInfo]
    ) -> Tuple[str, str]:
        """
        –≠–¢–ê–ü 2: –§–∞–∫—Ç—á–µ–∫–∏–Ω–≥ –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        """
        logger.info(f"üìä –≠–¢–ê–ü 2: –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–∫—Ç—ã –ø–æ {len(sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º...")

        if not sources:
            # –ï—Å–ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–µ –Ω—É–∂–Ω—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–ø–∞–º), –¥–µ–ª–∞–µ–º –±—ã—Å—Ç—Ä—É—é –ø—Ä–æ–≤–µ—Ä–∫—É
            return await self._quick_spam_check(text, debug)

        attempts = self._build_stage2_attempts(sources)
        last_error: Optional[Exception] = None

        for idx, attempt_sources in enumerate(attempts, start=1):
            logger.info(
                "üß™ –≠–¢–ê–ü 2: –ø–æ–ø—ã—Ç–∫–∞ %s —Å %s –¥–æ–º–µ–Ω–∞–º–∏", idx, len(attempt_sources)
            )

            if debug:
                debug.stage2_attempts += 1

            try:
                return await self._run_stage2_attempt(
                    text,
                    attempt_sources,
                    Config.FACT_CHECK_TIMEOUT,
                    analysis,
                    debug
                )
            except asyncio.TimeoutError:
                last_error = asyncio.TimeoutError()
                preview = [src.get("domain") or self._extract_domain(src.get("url")) or "?" for src in attempt_sources[:3]]
                preview_text = ", ".join(filter(None, preview))
                if len(attempt_sources) > 3:
                    preview_text += "..."
                logger.warning(
                    "‚è∞ –¢–∞–π–º–∞—É—Ç –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ %s —ç—Ç–∞–ø–∞ 2 (–¥–æ–º–µ–Ω—ã: %s)",
                    idx,
                    preview_text or "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
                )
                if debug:
                    base_reason = debug.reasoning if debug.reasoning else "–õ–æ–≥–∏–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"
                    debug.reasoning = f"{base_reason} (timeout –ø–æ–ø—ã—Ç–∫–∞ {idx})"
                continue
            except Exception as e:
                last_error = e
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —ç—Ç–∞–ø–∞ 2 –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {idx}: {e}")
                if "gpt-5" in str(e).lower() and self.gpt5_available:
                    logger.info("GPT-5 –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ GPT-4o")
                    self.gpt5_available = False
                    return await self._stage2_fact_check(text, sources, debug)
                if debug:
                    base_reason = debug.reasoning if debug.reasoning else "–õ–æ–≥–∏–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"
                    debug.reasoning = f"{base_reason} (–æ—à–∏–±–∫–∞ —ç—Ç–∞–ø–∞ 2, –ø–æ–ø—ã—Ç–∫–∞ {idx})"
                continue

        logger.warning("‚ö†Ô∏è –≠—Ç–∞–ø 2 –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –≤ fallback")
        if debug:
            debug.fallback_used = True
        if isinstance(last_error, asyncio.TimeoutError):
            if debug:
                base_reason = debug.reasoning if debug.reasoning else "–õ–æ–≥–∏–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"
                debug.reasoning = f"{base_reason} (stage2 timeout)"
        return await self._fallback_check(text, debug)
    
    async def _quick_spam_check(self, text: str, debug: Optional[DebugInfo]) -> Tuple[str, str]:
        """–ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ø–∞–º –±–µ–∑ –≤–µ–±-–ø–æ–∏—Å–∫–∞"""
        logger.info("‚ö° –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ø–∞–º...")
        
        try:
            response = await self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{
                    "role": "user", 
                    "content": f"–≠—Ç–æ —Å–ø–∞–º/—Ä–µ–∫–ª–∞–º–∞/–º—É—Å–æ—Ä? –û—Ç–≤–µ—Ç—å –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º (–¥–∞/–Ω–µ—Ç): {text[:200]}"
                }],
                max_completion_tokens=10,
                temperature=0.1
            )
            
            answer = response.choices[0].message.content.strip().lower()
            
            if "–¥–∞" in answer or "—Å–ø–∞–º" in answer:
                return "—Å–∫—Ä—ã—Ç–æ", "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –∫–∞–∫ —Å–ø–∞–º/—Ä–µ–∫–ª–∞–º–∞"
            else:
                return "–¥—Ä—É–≥–æ–µ", ""
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏: {e}")
            return "–¥—Ä—É–≥–æ–µ", ""
    
    async def _fallback_check(self, text: str, debug: Optional[DebugInfo]) -> Tuple[str, str]:
        """–†–µ–∑–µ—Ä–≤–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞"""
        logger.info("üîÑ –†–µ–∑–µ—Ä–≤–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞...")
        
        try:
            response = await self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{
                    "role": "user", 
                    "content": f"""
–ö—Ä–∞—Ç–∫–æ –æ—Ü–µ–Ω–∏ —ç—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ:
"{text}"

–≠—Ç–æ: 1) —Å–ø–∞–º/–º—É—Å–æ—Ä 2) –Ω–æ–≤–æ—Å—Ç–∏ 3) —Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏—è 4) –¥—Ä—É–≥–æ–µ
–û—Ç–≤–µ—Ç—å –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π: –∫–∞—Ç–µ–≥–æ—Ä–∏—è | –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π (–µ—Å–ª–∏ –Ω—É–∂–µ–Ω)
"""
                }],
                max_completion_tokens=50,
                temperature=0.1
            )
            
            answer = response.choices[0].message.content.strip()
            answer_lower = answer.lower()
            manual_review = "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏, —Ç—Ä–µ–±—É–µ—Ç—Å—è —Ä—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞"

            if "—Å–ø–∞–º" in answer_lower or "–º—É—Å–æ—Ä" in answer_lower:
                return "—Å–∫—Ä—ã—Ç–æ", "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –∫–∞–∫ —Å–ø–∞–º"

            if "—Ä–∞–∑–≤–ª–µ—á" in answer_lower:
                return "—Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏—è", manual_review

            if "–Ω–æ–≤–æ—Å—Ç" in answer_lower:
                return "–Ω–æ–≤–æ—Å—Ç–∏", manual_review

            if "–¥—Ä—É–≥–æ" in answer_lower:
                return "–¥—Ä—É–≥–æ–µ", manual_review

            return "–¥—Ä—É–≥–æ–µ", manual_review
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏: {e}")
            return "–¥—Ä—É–≥–æ–µ", "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏, —Ç—Ä–µ–±—É–µ—Ç—Å—è —Ä—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞"

    async def _run_stage2_attempt(
        self,
        text: str,
        attempt_sources: List[Dict[str, Any]],
        timeout: float,
        analysis: Optional[Dict[str, Any]],
        debug: Optional[DebugInfo]
    ) -> Tuple[str, str]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–¥–∏–Ω–æ—á–Ω—É—é –ø–æ–ø—ã—Ç–∫—É —ç—Ç–∞–ø–∞ 2 —Å –∑–∞–¥–∞–Ω–Ω—ã–º —Å–ø–∏—Å–∫–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤."""

        sources_text = self._format_sources_for_prompt(attempt_sources)

        queries = analysis.get("recommended_queries") if analysis else None
        queries_text = ""
        if queries:
            prepared = [q for q in queries[:3] if isinstance(q, str) and q.strip()]
            if prepared:
                # –û–±–Ω–æ–≤–ª—è–µ–º –≥–æ–¥—ã –≤ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö –Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω—ã–π
                updated_queries = self._update_queries_with_current_year(prepared)
                bullet_list = "\n".join([f"‚Ä¢ {q.strip()}" for q in updated_queries])
                queries_text = f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã:\n{bullet_list}\n\n"

        allowed_domains = [
            src.get("domain") or self._extract_domain(src.get("url"))
            for src in attempt_sources
        ]
        allowed_domains = [d for d in allowed_domains if d]

        # Check for X.com/Twitter domains
        x_domains = [d for d in allowed_domains if 'x.com' in d or 'twitter.com' in d]
        
        # Special instructions for X.com/Twitter searches
        x_instructions = ""
        if x_domains:
            x_instructions = f"""

SPECIAL INSTRUCTIONS FOR X.COM/TWITTER:
- Search for specific tweets, posts, and statements by the mentioned people
- Look for recent posts (last 24-48 hours) as well as older content
- Pay attention to verified accounts and official statements
- Search using various formats: direct quotes, paraphrases, key phrases
- Check replies and quote tweets for additional context
- If searching fails, explicitly state "X.com search limitations encountered"
"""

        prompt = f"""
You are a strict fact-checker. Verify this message using web search ONLY on the specified reliable sources.

Sources to check:
{sources_text}

{queries_text}
Message to verify: "{text}"

CRITICAL INSTRUCTIONS:
1. Search the specified domains for EXACT information matching the message
2. Verify EVERY specific claim, detail, and statement in the message
3. Pay special attention to precise wording (e.g., "will affect" vs "will NOT affect")
4. Look for direct quotes or official statements that confirm or contradict the claims
5. If any detail cannot be confirmed or contradicts found information, mark as unconfirmed/contradictory{x_instructions}

Response in strict JSON format:
{{
  "verification_status": "confirmed|partially_confirmed|contradictory|unconfirmed",
  "confidence_score": 75,
  "category": "news|entertainment|other|spam",
  "detailed_findings": "What exactly was found/not found in sources with specific details",
  "contradictions": "Any contradictions found between message and sources",
  "direct_quotes": ["Direct quotes from sources that support or contradict the message"],
  "sources_checked": ["List of sources actually checked"],
  "missing_evidence": "What specific claims lack evidence",
  "special_notes": "Any special circumstances like fresh content, API limitations, etc."
}}

CRITICAL: confidence_score MUST be a numeric integer between 0-100, NOT text like "ninety" or "high".

Verification criteria:
- "confirmed" (90-100): Direct quotes/official statements support ALL claims
- "partially_confirmed" (60-89): Some claims supported, others unclear  
- "contradictory" (30-59): Some claims directly contradicted by sources
- "unconfirmed" (0-29): No supporting evidence found for key claims
"""

        # Special logging for X.com searches
        x_domains = [d for d in allowed_domains if 'x.com' in d or 'twitter.com' in d]
        if x_domains:
            logger.info(f"üê¶ X.com –ø–æ–∏—Å–∫: –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–º–µ–Ω—ã {x_domains}")
            logger.info(f"üîç –ü–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã: {queries_text.strip() if queries_text else '–ù–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤'}")
            logger.info(f"üìù –¢–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏: {text[:100]}...")

        responses_client = self.client.responses

        try:
            create_task = responses_client.create(
                model=self.fact_check_model,
                tools=[{
                    "type": "web_search",
                    "filters": {
                        "allowed_domains": allowed_domains
                    }
                }],
                input=prompt,
                tool_choice="auto",
                max_output_tokens=Config.STAGE2_MAX_TOKENS
            )
            initial_response = await asyncio.wait_for(create_task, timeout=timeout)
            response = await self._poll_response(responses_client, initial_response, timeout)
        except asyncio.TimeoutError:
            raise
        except Exception as err:
            if "model" in str(err).lower() and "not supported" in str(err).lower():
                logger.warning(
                    "‚ö†Ô∏è –ú–æ–¥–µ–ª—å %s –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Responses API, –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ gpt-4o",
                    self.fact_check_model
                )
                self.fact_check_model = "gpt-4o"
                create_task = responses_client.create(
                    model=self.fact_check_model,
                    tools=[{
                        "type": "web_search",
                        "filters": {
                            "allowed_domains": allowed_domains
                        }
                    }],
                    input=prompt,
                    tool_choice="auto",
                    max_output_tokens=Config.STAGE2_MAX_TOKENS
                )
                initial_response = await asyncio.wait_for(create_task, timeout=timeout)
                response = await self._poll_response(responses_client, initial_response, timeout)
            else:
                raise

        if debug:
            debug.web_search_used = True

        output_text = self._extract_response_text(response)
        status = getattr(response, "status", None)
        logger.debug("üì∂ –°—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞ —ç—Ç–∞–ø–∞ 2: %s", status)
        if logger.isEnabledFor(logging.DEBUG):
            try:
                logger.debug("üìù –ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç —ç—Ç–∞–ø–∞ 2: %s", response.model_dump(exclude_none=True))
            except Exception:
                logger.debug("üìù –ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç —ç—Ç–∞–ø–∞ 2: %r", response)
        logger.info(f"üìÑ –û—Ç–≤–µ—Ç —ç—Ç–∞–ø–∞ 2: {output_text[:200]}...")
        
        # Special logging for X.com search results  
        x_domains = [d for d in allowed_domains if 'x.com' in d or 'twitter.com' in d]
        if x_domains:
            logger.info(f"üê¶ X.com —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {output_text[:300]}...")
            if 'sources_checked' in output_text.lower():
                try:
                    temp_result = json.loads(output_text if output_text.startswith('{') else output_text[output_text.find('{'):output_text.rfind('}')+1])
                    sources_checked = temp_result.get("sources_checked", [])
                    x_sources_found = [s for s in sources_checked if 'x.com' in str(s).lower() or 'twitter.com' in str(s).lower()]
                    logger.info(f"üê¶ X.com –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–∞–π–¥–µ–Ω—ã: {x_sources_found}")
                except:
                    logger.info("üê¶ X.com: –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å sources_checked –∏–∑ –æ—Ç–≤–µ—Ç–∞")

        if not output_text:
            raise ValueError("–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏ —ç—Ç–∞–ø–∞ 2")

        try:
            result = json.loads(output_text)
        except json.JSONDecodeError:
            json_start = output_text.find('{')
            json_end = output_text.rfind('}') + 1
            if json_start == -1 or json_end <= json_start:
                raise json.JSONDecodeError("JSON –Ω–µ –Ω–∞–π–¥–µ–Ω", output_text, 0)
            json_text = output_text[json_start:json_end]
            result = json.loads(json_text)

        # Handle new verification-based schema
        verification_status = result.get("verification_status", "")
        confidence_score = result.get("confidence_score", 0)
        
        # Validate and fix confidence_score if it's not numeric
        if not isinstance(confidence_score, (int, float)):
            logger.warning(f"‚ö†Ô∏è confidence_score –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —á–∏—Å–ª–æ–º: {confidence_score}, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º 0")
            confidence_score = 0
        else:
            confidence_score = int(confidence_score)
            if confidence_score < 0 or confidence_score > 100:
                logger.warning(f"‚ö†Ô∏è confidence_score –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ 0-100: {confidence_score}, –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º")
                confidence_score = max(0, min(100, confidence_score))
        category = result.get("category", "–¥—Ä—É–≥–æ–µ")
        
        # Check for spam category first
        if category == "spam":
            return "—Å–∫—Ä—ã—Ç–æ", "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –∫–∞–∫ —Å–ø–∞–º"
        
        # Fix confidence_score logic for contradictory status
        # If status is contradictory, confidence_score should reflect low trust in the claim
        if verification_status == "contradictory" and confidence_score > 50:
            # Invert confidence score - high model confidence in contradiction = low trust in claim
            confidence_score = 100 - confidence_score
            logger.info(f"üîÑ Inverted confidence_score for contradictory status: {confidence_score}%")
        
        # Extract fields from API response
        detailed_findings = result.get("detailed_findings", "")
        contradictions = result.get("contradictions", "")
        missing_evidence = result.get("missing_evidence", "")
        special_notes = result.get("special_notes", "")
        
        # Save all fields to debug_info
        if debug:
            debug.confidence_score = confidence_score
            debug.verification_status = verification_status
            debug.detailed_findings = detailed_findings
            debug.contradictions = contradictions
            debug.missing_evidence = missing_evidence
            debug.special_notes = special_notes
        
        # Stage 2.5: Translate comment fields to Russian if enabled
        await self._translate_comment_fields(debug)
        
        # Build comment from translated fields
        comment = self._build_translated_comment(verification_status, confidence_score, debug)
        
        return category, comment

    async def _translate_comment_fields(self, debug: Optional[DebugInfo]) -> None:
        """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø–æ–ª—è –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ (Stage 2.5)"""
        if not debug or not Config.TRANSLATE_TO_RUSSIAN:
            return
        
        logger.info("üåê STAGE 2.5: –ü–µ—Ä–µ–≤–æ–¥–∏–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –Ω–∞ —Ä—É—Å—Å–∫–∏–π...")
        
        # –ü–µ—Ä–µ–≤–æ–¥–∏–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø–æ–ª—è
        fields_to_translate = [
            ('detailed_findings', '–¥–µ—Ç–∞–ª—å–Ω—ã–µ –≤—ã–≤–æ–¥—ã'),
            ('contradictions', '–ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è'),
            ('missing_evidence', '–æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞'),
            ('special_notes', '—Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—á–∞–Ω–∏—è')
        ]
        
        for field_name, field_description in fields_to_translate:
            field_value = getattr(debug, field_name, "")
            if field_value and field_value.strip():
                try:
                    translated_text = await self._translate_text(field_value, field_description)
                    setattr(debug, field_name, translated_text)
                    logger.info(f"‚úÖ –ü–µ—Ä–µ–≤–µ–¥–µ–Ω–æ –ø–æ–ª–µ {field_name}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ –ø–æ–ª—è {field_name}: {e}")
                    # –û—Å—Ç–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ

    async def _translate_text(self, text: str, field_description: str = "—Ç–µ–∫—Å—Ç") -> str:
        """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏"""
        try:
            response = await self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{
                    "role": "user", 
                    "content": f"""–ü–µ—Ä–µ–≤–µ–¥–∏ —ç—Ç–æ—Ç {field_description} fact-checking —Å–∏—Å—Ç–µ–º—ã –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫. 
                    
–í–ê–ñ–ù–û:
- –°–æ—Ö—Ä–∞–Ω–∏ –≤—Å—é —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å
- –ü–µ—Ä–µ–≤–µ–¥–∏ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–∞ —Ä—É—Å—Å–∫–∏–π, –µ—Å–ª–∏ —ç—Ç–æ –æ–±—â–µ–ø—Ä–∏–Ω—è—Ç–æ
- –°–æ—Ö—Ä–∞–Ω–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ –¥–∞—Ç—ã
- –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–æ–Ω

–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç:
{text}

–ü–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:"""
                }],
                max_completion_tokens=500,
                temperature=0.1,
                timeout=10
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
            return text  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –ø—Ä–∏ –æ—à–∏–±–∫–µ

    def _build_translated_comment(self, verification_status: str, confidence_score: int, debug: Optional[DebugInfo]) -> str:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —É–∂–µ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã—Ö –ø–æ–ª–µ–π –∏–∑ debug_info"""
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–µ –ø–æ–ª—è –∏–∑ debug_info (–µ—Å–ª–∏ –µ—Å—Ç—å)
        detailed_findings = debug.detailed_findings if debug else ""
        contradictions = debug.contradictions if debug else ""
        missing_evidence = debug.missing_evidence if debug else ""
        special_notes = debug.special_notes if debug else ""
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ verification_status
        if verification_status == "confirmed" and confidence_score >= 90:
            comment = "–î–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ"
            if detailed_findings:
                comment += f" - {detailed_findings}"
        elif verification_status == "partially_confirmed" and confidence_score >= 60:
            comment = "–ß–∞—Å—Ç–∏—á–Ω–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ"
            if detailed_findings:
                comment += f" - {detailed_findings}"
            elif contradictions:
                comment += f" - –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –¥–µ—Ç–∞–ª–∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç: {contradictions}"
        elif verification_status == "contradictory":
            comment = "–ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º"
            if contradictions:
                comment += f" - {contradictions}"
            elif detailed_findings:
                comment += f" - {detailed_findings}"
        elif verification_status == "unconfirmed" or confidence_score < 30:
            comment = "–ù–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ"
            if missing_evidence:
                comment += f" - {missing_evidence}"
            elif detailed_findings:
                comment += f" - {detailed_findings}"
        else:
            # Fallback for edge cases
            comment = "–¢—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏"
            if detailed_findings:
                comment += f" - {detailed_findings}"
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—á–∞–Ω–∏—è –µ—Å–ª–∏ –µ—Å—Ç—å
        if special_notes and special_notes.strip():
            comment += f" [–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: {special_notes}]"
        
        return comment

    def _build_stage2_attempts(self, sources: List[Dict[str, Any]]) -> List[List[Dict[str, Any]]]:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ–ø—ã—Ç–æ–∫ –¥–ª—è —ç—Ç–∞–ø–∞ 2 —Å —Ä–∞–∑–Ω—ã–º–∏ –ª–∏–º–∏—Ç–∞–º–∏ –¥–æ–º–µ–Ω–æ–≤."""

        unique_sources: List[Dict[str, Any]] = []
        seen = set()
        for candidate in sources:
            domain = candidate.get("domain") or self._extract_domain(candidate.get("url"))
            url = candidate.get("url") or ""
            if not domain and not url:
                continue
            key = (domain or "", url)
            if key in seen:
                continue
            seen.add(key)
            normalized = dict(candidate)
            normalized["domain"] = domain
            if url:
                normalized["url"] = url
            elif domain:
                normalized["url"] = f"https://{domain}"
            unique_sources.append(normalized)

        if not unique_sources:
            return [[]]

        limits: List[int] = []
        if Config.STAGE2_INITIAL_DOMAIN_LIMIT:
            limits.append(Config.STAGE2_INITIAL_DOMAIN_LIMIT)
        if Config.STAGE2_RETRY_DOMAIN_LIMIT and Config.STAGE2_RETRY_DOMAIN_LIMIT != Config.STAGE2_INITIAL_DOMAIN_LIMIT:
            limits.append(Config.STAGE2_RETRY_DOMAIN_LIMIT)
        limits.append(len(unique_sources))

        attempts: List[List[Dict[str, Any]]] = []
        for limit in limits:
            if limit is None or limit <= 0:
                continue
            trimmed = unique_sources[:limit]
            if not trimmed:
                continue
            if trimmed not in attempts:
                attempts.append(trimmed)

        if not attempts:
            attempts.append(unique_sources)
        elif attempts[-1] != unique_sources:
            attempts.append(unique_sources)

        return attempts

    def _needs_fact_check(self, analysis: Dict[str, Any]) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤—Ç–æ—Ä–æ–≥–æ —ç—Ç–∞–ø–∞ –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –∞–Ω–∞–ª–∏–∑–∞ —ç—Ç–∞–ø–∞ 1."""
        if "needs_fact_check" in analysis:
            return bool(analysis.get("needs_fact_check"))

        topic_type = (analysis.get("classification") or analysis.get("topic_type") or "").lower()

        if topic_type in {"spam", "entertainment"}:
            return False

        if topic_type == "personal":
            return False

        return True

    def _finalize_without_stage2(self, analysis: Dict[str, Any]) -> Tuple[str, str]:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç –∏—Ç–æ–≥ –±–µ–∑ –∑–∞–ø—É—Å–∫–∞ –≤—Ç–æ—Ä–æ–≥–æ —ç—Ç–∞–ø–∞ —Ñ–∞–∫—Ç—á–µ–∫–∏–Ω–≥–∞."""
        classification = (analysis.get("classification") or analysis.get("topic_type") or "").lower()
        skip_reason = analysis.get("skip_reason") or analysis.get("reasoning") or "–ù–µ —Ç—Ä–µ–±—É–µ—Ç —Ñ–∞–∫—Ç—á–µ–∫–∏–Ω–≥–∞"

        if classification == "spam":
            return "—Å–∫—Ä—ã—Ç–æ", skip_reason or "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –∫–∞–∫ —Å–ø–∞–º"

        if classification == "entertainment":
            return "—Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏—è", skip_reason if skip_reason else ""

        if classification == "personal":
            return "–¥—Ä—É–≥–æ–µ", skip_reason or "–ù–µ–ø—Ä–æ–≤–µ—Ä—è–µ–º–æ–µ –ª–∏—á–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ"

        if classification == "news":
            return "–Ω–æ–≤–æ—Å—Ç–∏", skip_reason or "–ù–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏"

        return "–¥—Ä—É–≥–æ–µ", skip_reason or "–ù–µ —Ç—Ä–µ–±—É–µ—Ç —Ñ–∞–∫—Ç—á–µ–∫–∏–Ω–≥–∞"

    def _normalize_candidates(self, candidates: Any) -> List[Dict[str, Any]]:
        """–ü—Ä–∏–≤–æ–¥–∏—Ç —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∫ –µ–¥–∏–Ω–æ–º—É –≤–∏–¥—É."""

        if not isinstance(candidates, list):
            return []

        normalized: List[Dict[str, Any]] = []

        for item in candidates:
            if isinstance(item, dict):
                url = item.get("url") or item.get("link") or ""
                domain = item.get("domain") or self._extract_domain(url)
                if not domain and not url:
                    continue
                name = item.get("name") or domain or url
                why = item.get("why") or item.get("reason") or item.get("note", "")
                priority = item.get("priority")
                try:
                    priority = int(priority)
                except (TypeError, ValueError):
                    priority = len(normalized) + 1
                normalized.append({
                    "name": name,
                    "url": url or (f"https://{domain}" if domain else ""),
                    "domain": domain,
                    "why": why,
                    "priority": priority
                })
            elif isinstance(item, str):
                domain = self._extract_domain(item)
                url = item if item.startswith("http") else (f"https://{item}" if domain else "")
                normalized.append({
                    "name": domain or item,
                    "url": url,
                    "domain": domain,
                    "why": "–ò—Å—Ç–æ—á–Ω–∏–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–æ–¥–µ–ª—å—é",
                    "priority": len(normalized) + 1
                })

        filtered = [src for src in normalized if src.get("domain") or src.get("url")]
        filtered.sort(key=lambda s: s.get("priority", 999))
        return filtered[:Config.MAX_SOURCE_DOMAINS]

    def _build_backup_sources(self, text: str) -> List[Dict[str, Any]]:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç —Ä–µ–∑–µ—Ä–≤–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""

        fallback_domains = list(self.sources.get_category_domains("general_news"))
        fallback_domains += list(self.sources.get_sources_for_topic(text))

        unique: List[Dict[str, Any]] = []
        seen = set()
        for domain in fallback_domains:
            if not domain or domain in seen:
                continue
            seen.add(domain)
            unique.append({
                "name": domain,
                "url": f"https://{domain}",
                "domain": domain,
                "why": "–†–µ–∑–µ—Ä–≤–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫",
                "priority": len(unique) + 1
            })

        return unique[:Config.MAX_SOURCE_DOMAINS]

    def _extract_domain(self, value: Optional[str]) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ–º–µ–Ω –∏–∑ URL –∏–ª–∏ —Å—ã—Ä–æ–π —Å—Ç—Ä–æ–∫–∏."""

        if not value:
            return None

        candidate = value.strip()
        if not candidate or " " in candidate:
            return None

        parsed = urlparse(candidate if candidate.startswith("http") else f"https://{candidate}")
        domain = parsed.netloc.lower()
        if domain.startswith("www."):
            domain = domain[4:]
        return domain or None

    def _update_queries_with_current_year(self, queries: List[str]) -> List[str]:
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã, –∑–∞–º–µ–Ω—è—è —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –≥–æ–¥—ã –Ω–∞ —Ç–µ–∫—É—â–∏–π –≥–æ–¥."""
        current_year = datetime.now().year
        updated_queries = []
        
        for query in queries:
            # –ó–∞–º–µ–Ω—è–µ–º –≥–æ–¥—ã –æ—Ç 2020 –¥–æ —Ç–µ–∫—É—â–µ–≥–æ –≥–æ–¥–∞-1 –Ω–∞ —Ç–µ–∫—É—â–∏–π –≥–æ–¥
            # –ü—Ä–∏–º–µ—Ä—ã: "–ö—Ä—ã–º –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å 2023" -> "–ö—Ä—ã–º –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å 2025"
            updated_query = query
            for old_year in range(2020, current_year):
                if str(old_year) in query:
                    updated_query = updated_query.replace(str(old_year), str(current_year))
                    logger.info(f"üóìÔ∏è –û–±–Ω–æ–≤–ª–µ–Ω –≥–æ–¥ –≤ –∑–∞–ø—Ä–æ—Å–µ: {old_year} -> {current_year}")
                    break
            
            updated_queries.append(updated_query)
        
        return updated_queries

    def _format_sources_for_prompt(self, sources: List[Dict[str, Any]]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ –º–æ–¥–µ–ª—å."""

        lines: List[str] = []
        for src in sources[:15]:
            name = src.get("name") or src.get("domain") or src.get("url")
            url = src.get("url") or (f"https://{src.get('domain')}" if src.get("domain") else "")
            why = src.get("why")
            segments = [segment for segment in [name, url, why] if segment]
            if segments:
                lines.append("‚Ä¢ " + " ‚Äî ".join(segments))

        if not lines:
            return "‚Ä¢ (–∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã)"

        return "\n".join(lines)

    def _extract_response_text(self, response: Any) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ –æ–±—ä–µ–∫—Ç–∞ –æ—Ç–≤–µ—Ç–∞ Responses API."""

        direct_text = getattr(response, "output_text", None)
        if direct_text:
            return direct_text.strip()

        data: Dict[str, Any] = {}
        if hasattr(response, "model_dump"):
            try:
                data = response.model_dump(exclude_none=True)
            except Exception:
                data = {}
        elif hasattr(response, "dict"):
            try:
                data = response.dict(exclude_none=True)
            except Exception:
                data = {}

        chunks: List[str] = []
        for item in data.get("output", []) or []:
            if isinstance(item, dict):
                item_type = item.get("type")
                if item_type == "message":
                    content = item.get("content") or []
                    chunks.extend(self._extract_text_from_content(content))
                elif item_type == "tool_call":
                    tool = item.get("tool_call") or {}
                    output = tool.get("output") or tool.get("result") or tool.get("response")
                    chunks.extend(self._extract_text_from_tool_output(output))
            else:
                content = getattr(item, "content", None)
                if content:
                    chunks.extend(self._extract_text_from_content(content))

        if chunks:
            return "\n".join(chunks).strip()

        # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ ‚Äî –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–ª–æ –æ—Ç–≤–µ—Ç–∞ —Ü–µ–ª–∏–∫–æ–º
        raw = data.get("response") or data.get("output_text")
        if isinstance(raw, str) and raw.strip():
            return raw.strip()

        return ""

    def _extract_text_from_content(self, content: Any) -> List[str]:
        """–í—ã—Ç—è–≥–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ message."""

        segments: List[str] = []
        for part in content:
            if isinstance(part, dict):
                text_value = part.get("text")
                if text_value:
                    segments.append(text_value)
            else:
                text_value = getattr(part, "text", None)
                if text_value:
                    segments.append(text_value)
        return segments

    def _parse_stage1_json(self, payload: str) -> Optional[Dict[str, Any]]:
        """Simple JSON parsing with fallback to None on truncation."""
        if not payload:
            return None
        
        try:
            return json.loads(payload)
        except json.JSONDecodeError as e:
            logger.warning(f"‚ö†Ô∏è STAGE1: Failed to parse JSON (likely truncated): {str(e)[:100]}")
            logger.debug(f"Truncated JSON: {payload[:300]}")
            return None


    def _extract_text_from_tool_output(self, output: Any) -> List[str]:
        """–î–æ—Å—Ç–∞–µ—Ç —á–∏—Ç–∞–µ–º—ã–π —Ç–µ–∫—Å—Ç –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞."""

        segments: List[str] = []
        if isinstance(output, str):
            segments.append(output)
        elif isinstance(output, list):
            for item in output:
                if isinstance(item, dict):
                    if item.get("type") == "text" and item.get("text"):
                        segments.append(item["text"])
                    elif item.get("title") or item.get("url") or item.get("snippet"):
                        title = item.get("title")
                        snippet = item.get("snippet")
                        url = item.get("url")
                        pieces = [piece for piece in [title, snippet, url] if piece]
                        if pieces:
                            segments.append(" ‚Äî ".join(pieces))
                elif isinstance(item, str):
                    segments.append(item)
        elif isinstance(output, dict):
            if output.get("text"):
                segments.append(output["text"])
            if output.get("content"):
                segments.extend(self._extract_text_from_tool_output(output["content"]))
        return segments

    async def _poll_response(self, responses_client, response: Any, timeout: float) -> Any:
        """–û–∂–∏–¥–∞–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è Responses API —Å —Ç–∞–π–º–∞—É—Ç–æ–º."""

        start = time.time()
        current = response

        while getattr(current, "status", "completed") in {"in_progress", "queued", "requires_action"}:
            remaining = timeout - (time.time() - start)
            if remaining <= 0:
                raise asyncio.TimeoutError("Polling timed out")
            await asyncio.sleep(min(1.5, remaining))
            current = await responses_client.get(current.id)

        return current

    async def _stage1_retry_prompt(self, text: str) -> Optional[Dict[str, Any]]:
        """–ü–æ–≤—Ç–æ—Ä–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è —ç—Ç–∞–ø–∞ 1 —Å —É–ø—Ä–æ—â—ë–Ω–Ω—ã–º–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏."""

        retry_prompt = f"""
–í–µ—Ä–Ω–∏ –≤–∞–ª–∏–¥–Ω—ã–π JSON (–¥–æ 6 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤) –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤ –ø–æ —Å–æ–æ–±—â–µ–Ω–∏—é:
"{text}"

–°—Ç—Ä–æ–≥–æ —Å–ª–µ–¥—É–π —Ñ–æ—Ä–º–∞—Ç—É:
{{
  "needs_fact_check": true/false,
  "classification": "news/entertainment/personal/spam/other",
  "reasoning": "...",
  "skip_reason": "...",
  "source_candidates": [
    {{"name": "...", "url": "https://...", "domain": "...", "why": "...", "priority": 1}}
  ],
  "recommended_queries": ["..."]
}}

–ù–∏–∫–∞–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤–Ω–µ JSON.
"""

        try:
            response = await self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": retry_prompt}],
                max_completion_tokens=400,
                temperature=0.0,
                response_format={"type": "json_object"}
            )
            result_text = response.choices[0].message.content.strip()
            logger.info(f"üìã –û—Ç–≤–µ—Ç —ç—Ç–∞–ø–∞ 1 (retry): {result_text}")
            return self._parse_stage1_json(result_text)
        except Exception as err:
            logger.error(f"‚ùå –≠–¢–ê–ü 1 retry –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –æ—à–∏–±–∫–æ–π: {err}")
            return None
